{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__AUSTIN BLUETOOTH SENSOR DATA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time as t\n",
    "import sys\n",
    "import csv, sqlite3\n",
    "import os.path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a dataframe with Origin, Destination, Time, Weekday, Year, Samples\n",
    "# and Average time*Sample, total of 7 columns\n",
    "def getTimeWeekdayYear(df):\n",
    "    weekday = []  # range [0, 6], Monday is 0\n",
    "    year = []\n",
    "    time = [] # from 0 - 2399\n",
    "    ts = df.timestamp\n",
    "    row_count = 0\n",
    "    for row in ts.values:\n",
    "        # dt = datetime.datetime.strptime(row, '%m/%d/%Y %I:%M:%S %p')\n",
    "        # time.append(datetime.datetime.strftime(dt, '%H:%M'))\n",
    "\n",
    "        strip_time = t.strptime(row, '%m/%d/%Y %I:%M:%S %p')\n",
    "        hour = strip_time.tm_hour\n",
    "        minute = strip_time.tm_min\n",
    "        time.extend([hour*60+minute])\n",
    "        weekday.extend([strip_time.tm_wday])\n",
    "        year.extend([strip_time.tm_year])\n",
    "\n",
    "        row_count += 1\n",
    "        if row_count % 100000 == 0:\n",
    "            print \"Processing row \" + str(row_count)\n",
    "\n",
    "    print \"Total row count: \" + str(row_count) + ' rows\\n'\n",
    "\n",
    "    avg_time_mul_sample = df.average_travel_time_seconds * df.number_samples\n",
    "\n",
    "    df2 = pd.DataFrame({\"Origin\": df.origin_reader_identifier, \"Destination\": df.destination_reader_identifier,\n",
    "                        'Time': time, 'Year': year, 'Weekday': weekday,\n",
    "                        'Samples': df.number_samples, \"time_mul_sample\": avg_time_mul_sample})\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df2 = getTimeWeekdayYear(df)\n",
    "\n",
    "    aggregations = {\n",
    "        'time_mul_sample': {\n",
    "            'sum_time_mul_sample': 'sum'\n",
    "        },\n",
    "        'Samples': {\n",
    "            'total_sample': 'sum'\n",
    "        }\n",
    "\n",
    "    }\n",
    "    df3 = df2.groupby(['Origin', 'Destination', 'Year', 'Weekday', 'Time']).agg(aggregations).reset_index()\n",
    "    df3.columns = ['Origin', 'Destination', 'Year', 'Weekday', 'Time', \"Total_sample\", \"sum_time_mul_sample\"]\n",
    "\n",
    "    avg_travel_time = df3.sum_time_mul_sample/df3.Total_sample\n",
    "\n",
    "    df4 = pd.DataFrame({\"Origin\": df3.Origin, \"Destination\": df3.Destination,\n",
    "                        'Year': df3.Year, 'Weekday': df3.Weekday, 'Time': df3.Time,\n",
    "                        'Avg_travel_time': avg_travel_time.apply(int)})\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot find preprocessed_summary.csv, creating a new one from TMSR...\n",
      "Processing row 100000\n",
      "Processing row 200000\n",
      "Processing row 300000\n",
      "Processing row 400000\n",
      "Processing row 500000\n",
      "Processing row 600000\n",
      "Processing row 700000\n",
      "Processing row 800000\n",
      "Processing row 900000\n",
      "Processing row 1000000\n",
      "Processing row 1100000\n",
      "Processing row 1200000\n",
      "Processing row 1300000\n",
      "Processing row 1400000\n",
      "Processing row 1500000\n",
      "Processing row 1600000\n",
      "Processing row 1700000\n",
      "Processing row 1800000\n",
      "Processing row 1900000\n",
      "Processing row 2000000\n",
      "Processing row 2100000\n",
      "Processing row 2200000\n",
      "Processing row 2300000\n",
      "Processing row 2400000\n",
      "Processing row 2500000\n",
      "Processing row 2600000\n",
      "Processing row 2700000\n",
      "Processing row 2800000\n",
      "Processing row 2900000\n",
      "Processing row 3000000\n",
      "Processing row 3100000\n",
      "Processing row 3200000\n",
      "Processing row 3300000\n",
      "Processing row 3400000\n",
      "Processing row 3500000\n",
      "Processing row 3600000\n",
      "Processing row 3700000\n",
      "Processing row 3800000\n",
      "Processing row 3900000\n",
      "Processing row 4000000\n",
      "Processing row 4100000\n",
      "Processing row 4200000\n",
      "Processing row 4300000\n",
      "Processing row 4400000\n",
      "Processing row 4500000\n",
      "Processing row 4600000\n",
      "Processing row 4700000\n",
      "Processing row 4800000\n",
      "Processing row 4900000\n",
      "Processing row 5000000\n",
      "Processing row 5100000\n",
      "Processing row 5200000\n",
      "Processing row 5300000\n",
      "Processing row 5400000\n",
      "Processing row 5500000\n",
      "Processing row 5600000\n",
      "Processing row 5700000\n",
      "Processing row 5800000\n",
      "Processing row 5900000\n",
      "Processing row 6000000\n",
      "Processing row 6100000\n",
      "Processing row 6200000\n",
      "Processing row 6300000\n",
      "Processing row 6400000\n",
      "Processing row 6500000\n",
      "Processing row 6600000\n",
      "Processing row 6700000\n",
      "Processing row 6800000\n",
      "Processing row 6900000\n",
      "Processing row 7000000\n",
      "Processing row 7100000\n",
      "Processing row 7200000\n",
      "Processing row 7300000\n",
      "Processing row 7400000\n",
      "Processing row 7500000\n",
      "Processing row 7600000\n",
      "Processing row 7700000\n",
      "Processing row 7800000\n",
      "Processing row 7900000\n",
      "Processing row 8000000\n",
      "Processing row 8100000\n",
      "Processing row 8200000\n",
      "Processing row 8300000\n",
      "Processing row 8400000\n",
      "Processing row 8500000\n",
      "Processing row 8600000\n",
      "Processing row 8700000\n",
      "Processing row 8800000\n",
      "Processing row 8900000\n",
      "Processing row 9000000\n",
      "Processing row 9100000\n",
      "Processing row 9200000\n",
      "Processing row 9300000\n",
      "Processing row 9400000\n",
      "Processing row 9500000\n",
      "Processing row 9600000\n",
      "Processing row 9700000\n",
      "Processing row 9800000\n",
      "Processing row 9900000\n",
      "Processing row 10000000\n",
      "Processing row 10100000\n",
      "Processing row 10200000\n",
      "Processing row 10300000\n",
      "Processing row 10400000\n",
      "Processing row 10500000\n",
      "Processing row 10600000\n",
      "Processing row 10700000\n",
      "Processing row 10800000\n",
      "Processing row 10900000\n",
      "Processing row 11000000\n",
      "Processing row 11100000\n",
      "Processing row 11200000\n",
      "Processing row 11300000\n",
      "Processing row 11400000\n",
      "Processing row 11500000\n",
      "Processing row 11600000\n",
      "Processing row 11700000\n",
      "Processing row 11800000\n",
      "Processing row 11900000\n",
      "Processing row 12000000\n",
      "Total row count: 12006783 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manojgedela/anaconda2/lib/python2.7/site-packages/pandas/core/groupby.py:4291: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing data!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # start_time = t.time()\n",
    "    # Process Travel_Sensors.csv\n",
    "    con = sqlite3.connect(\"database.db\")\n",
    "    cur = con.cursor()\n",
    "\n",
    "    #TODO make a promt here to confirm if user really wants to create a new table.\n",
    "\n",
    "    # Drop table if the table exists\n",
    "    drop_travel_sensor_table_query = \"DROP TABLE IF EXISTS TravelSensor\"\n",
    "    cur.execute(drop_travel_sensor_table_query)\n",
    "\n",
    "    create_travel_sensor_table_query = \"              \\\n",
    "        CREATE TABLE IF NOT EXISTS TravelSensor(\\\n",
    "        ID          INT PRIMARY KEY NOT NULL,   \\\n",
    "        READER_ID   TEXT            NOT NULL,   \\\n",
    "        LATITUDE    REAL            NOT NULL,   \\\n",
    "        LONGITUDE   REAL            NOT NULL    \\\n",
    "        )                                       \\\n",
    "    \"\n",
    "    cur.execute(create_travel_sensor_table_query)\n",
    "\n",
    "    travel_sensors_csv = \"Travel_Sensors.csv\"\n",
    "    with open(travel_sensors_csv,'rb') as fin: # `with` statement available in 2.5+\n",
    "        # csv.DictReader uses first line in file for column headings by default\n",
    "        data_row = csv.reader(fin) # comma is default delimiter\n",
    "        to_db = [(i[1], i[0], i[28], i[29]) for i in data_row]\n",
    "\n",
    "    cur.executemany(\"INSERT INTO TravelSensor (ID, READER_ID, LATITUDE, LONGITUDE) VALUES (?, ?, ?, ?);\", to_db)\n",
    "    con.commit()\n",
    "\n",
    "    drop_summary_table_query = \"DROP TABLE IF EXISTS Summary\"\n",
    "    cur.execute(drop_summary_table_query)\n",
    "\n",
    "    create_summary_table_query = \"          \\\n",
    "        CREATE TABLE IF NOT EXISTS Summary( \\\n",
    "        Id INT PRIMARY KEY NOT NULL,        \\\n",
    "        Avg_Travel_Time REAL NOT NULL,      \\\n",
    "        Destination TEXT NOT NULL,          \\\n",
    "        Origin TEXT NOT NULL,               \\\n",
    "        Time INT NOT NULL,                  \\\n",
    "        Weekday INT NOT NULL,               \\\n",
    "        Year INT NOT NULL                   \\\n",
    "        )                                   \\\n",
    "    \"\n",
    "    cur.execute(create_summary_table_query)\n",
    "\n",
    "    summary_csv = \"preprocessed_summary.csv\"\n",
    "    if not os.path.isfile(summary_csv) :\n",
    "        print \"Cannot find preprocessed_summary.csv, creating a new one from TMSR...\"\n",
    "        # Download .csv file from https://data.austintexas.gov/dataset/Travel-Sensors-Match-Summary-Records/v7zg-5jg9/data\n",
    "        TMSR_csv = \"Bluetooth_Travel_Sensors_-Traffic_Match_Summary_Records__TMSR_.csv\"\n",
    "\n",
    "        df = pd.read_csv( TMSR_csv, dtype={\n",
    "            'timestamp': np.object,\n",
    "            'average_travel_time_seconds': np.int64,\n",
    "            'number_samples': np.int64,\n",
    "            'origin_reader_identifier': np.object,\n",
    "            'destination_reader_identifier': np.object\n",
    "            })\n",
    "\n",
    "        df4 = preprocess(df)\n",
    "        df4.to_csv(summary_csv, index=True, index_label=\"index\")\n",
    "        print \"Finished preprocessing data!\"\n",
    "\n",
    "    with open(summary_csv,'rb') as fin: # `with` statement available in 2.5+\n",
    "        # csv.DictReader uses first line in file for column headings by default\n",
    "        data_row = csv.reader(fin) # comma is default delimiter\n",
    "        # TODO use number index instead of string in i[]\n",
    "        to_db = [(i[0], i[1], i[2], i[3], i[4], i[5], i[6]) for i in data_row]\n",
    "\n",
    "    cur.executemany(\"INSERT INTO Summary (Id, Avg_Travel_Time, Destination, Origin, Time, Weekday, Year) VALUES (?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "    con.commit()\n",
    "\n",
    "    con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Column names and data types for Bluetooth_Travel_Sensors_-Traffic_Match_Summary_Records__TMSR.csv\n",
    "    # record_id                         object\n",
    "    # origin_reader_identifier          object\n",
    "    # destination_reader_identifier     object\n",
    "    # origin_roadway                    object\n",
    "    # origin_cross_street               object\n",
    "    # origin_direction                  object\n",
    "    # destination_roadway               object\n",
    "    # destination_cross_street          object\n",
    "    # destination_direction             object\n",
    "    # segment_length_miles             float64\n",
    "    # timestamp                         object\n",
    "    # average_travel_time_seconds        int64\n",
    "    # average_speed_mph                  int64\n",
    "    # summary_interval_minutes           int64\n",
    "    # number_samples                     int64\n",
    "    # standard_deviation               float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
